---
title: "Monte Carlo error"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Let's construct a table of parameters leading to different Markov chains, each having the same marginal distribution $N(0,1)$ at the limit of large number of samples but each also having different amount of autocorrelation between the samples.

```{r}
varTheta = 1
sigma2.1 = 1
sigma2.3 = 0.2
phi.2 = 0.5
phi.4 = 0.1
phi.1 = sqrt(1-sigma2.1/varTheta)
phi.3 = sqrt(1-sigma2.3/varTheta)
sigma2.2 = varTheta*(1-phi.2^2)
sigma2.4 = varTheta*(1-phi.4^2)
table.entries = matrix(nrow=4, ncol=4, data=c(
  varTheta, phi.1, sigma2.1, phi.1,
  varTheta, phi.2, sigma2.2, phi.2,
  varTheta, phi.3, sigma2.3, phi.3,
  varTheta, phi.4, sigma2.4, phi.4
))
table.entries <- t(table.entries)  # take transpose since matrix fills in the elements in columnwise
colnames(table.entries) <- c("var(theta)", "phi", "sigma2","corr")
print(table.entries)

```

Let's then construct a function to perform Markov chain sampling

```{r}
# let's first define a function to conduct the sampling
MarkovChain <- function(phi,sigma2,initial,m){
  theta = vector(length=m)
  theta[1] = initial
  for (i1 in seq(1,m,1)){
    theta[i1+1] = phi*theta[i1] + rnorm(1,0,sqrt(sigma2))
  }
  return(theta)
}
```

For this exercise it is handy to use multidimensional arrays to store the results (not necessary but saves some lines of code). Below an example:

```{r}
arr = array(dim=c(3,2,5))
dim(arr)

arr[1,1,] = 1
arr[1,2,] = 2
arr[3,2,] = 3
arr
```

Now we need to sample 100 independent realizations of length 2000 chains from the Markov chain defined in exercise 3.1 (that is; $\theta^{(1)},\dots, \theta^{(2000)}$) using each of the combinations of $\phi$ and $\sigma^2$ in the rows of the above table. 

With each of the chains we approximate $E[\theta^{(i)}]$, $\text{Pr}(\theta^{(i)}>0.5)$ and $\text{Pr}(\theta^{(i)}>2)$ using Monte Carlo with the $n=10$, $n=100$ and $n=1000$ last samples. Hence, we will construct 100 independent Monte Carlo approximations for the mean and two probabilities of $\theta$ corresponding to Markov chain sample sizes 10, 100 and 1000.

For example the below rows would construct two independent Markov chains of lenght 2000 and calculate the Monte Carlo approximation for the mean with the last 10 samples

```{r}
i1=1
m=2000
initial = 0
n=10
theta1 = MarkovChain(table.entries[i1,"phi"],table.entries[i1,"sigma2"],initial,m)  # sample a Markov chain
theta2 = theta = MarkovChain(table.entries[i1,"phi"],table.entries[i1,"sigma2"],initial,m)  # sample a Markov chain
mean(theta1[(m-n+1):m])
mean(theta2[(m-n+1):m])
```

Now, we need to repeat the above steps 100 times, calculate the mean and asked probabilities for each of the 100 chains and then examine how these Monte Carlo estimates behave and match with the exact results as we vary the row of the table and $n$. 

```{r}

res = array(dim=c(4,100,2001))
for (i in seq(1,4)) {
  phi = table.entries[i, 'phi']
  sigma2 = table.entries[i, 'sigma2']
  for (j in seq(1, 100)) {
    res[i,j,] = MarkovChain(table.entries[i1,"phi"],table.entries[i1,"sigma2"],initial,m)
  }
}
```
```{r}
for (i in seq(1, 5)) {
  par(mfrow=c(1, 3))
  mus = array(dim=c(3, 100))
  prob_half = array(dim=c(3, 100))
  prob_two = array(dim=c(3, 100))
  for (n in c(10, 100, 1000)) {
    for (j in seq(1, 100)) {
      if (i < 4) {
        mus[log(n, 10), j] = mean(tail(res[i,j,], n))
        prob_half[log(n, 10), j] = sum(tail(res[i,j,], n) > .5) / n
        prob_two[log(n, 10), j] = sum(tail(res[i,j,], n) > 2) / n
      } else {
        # do a sample from normal distribution for comparison
        norm_sample = rnorm(n)
        mus[log(n, 10), j] = mean(norm_sample)
        prob_half[log(n, 10), j] = sum(norm_sample > .5) / n
        prob_two[log(n, 10), j] = sum(norm_sample > 2) / n
      }
    }
  }
  for (n in seq(1,3)){
    hist(mus[n,], 15, c='pink', main=paste('Mus for index =', i, 'and n =', 10^n))
    hist(prob_half[n,], 15, main='theta > .5', c='salmon')
    hist(prob_two[n,], 15, main='theta > 2', c='red')
  }
  
}
```
We can see that as n is increased, the distributions are closer to the mean expected mean of 0, in general. Though if the autocorrelation has a large value, increasing n does not exactly improve this precision as the samples we take are not necessarily converged to the values of the normal distribution and this can be seen as a larger spread.

For the prob > .5 it seems that the smaller n is the larger spread for the probability is, but it generally seems to be distributed around the expected value of around 0.3. Telling whether the autocorrelation causes worse plots is kind of tough, it would seem that the probability distributions seem kind of similar.

For prob > 2 the histograms for smaller values of n are not very representative of the real expected distributions. This is most likely due to the fact that these values are quite unlikely being really centered around 0.02. But as n is increased the distribution centers around the real value. Here again the autocorrelation does not seem to terribly affect the end result, in the third plot perhaps the distribution is not as centered as you would expect but generally it would be a good estimate still.

I would say that smaller n causes the plots about rare values to not be very representative of true values but as n is increased, the distributions look correct. Having large autocorrelation can let in invalid values if values are taken too close to the beginning of the sampling, but this get fixed as the chains proceed.



