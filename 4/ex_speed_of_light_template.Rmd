---
title: "Speed of light data analysis"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions

Here we redo the analysis from page 66 in BDA3. The data are available from ex_speedOfLight.dat.

Simon Newcomb conducted experiments on speed of light in 1882. He measured the time required for 
light to travel a certain distance and here we will analyze a data recorded as deviations from $24,\!800$
nanoseconds.  The model used in BDA3 is 
%
\begin{align*}
y_i &\sim N(\mu, \sigma^2) \\
p(\mu,\sigma^2) &\propto \sigma^{-2}.
\end{align*}
%
where $y_i$ is the $i$'th measurement, $\mu$ is the mean of the measurement and $\sigma^{2}$ 
the variance of the measurements. Notice that this prior is improper ("uninformative"). 
This corresponds to widely used uniform prior for $\mu$ in the range $(-\infty,\infty)$, and uniform prior for $\log(\sigma)$ (BDA3 pp. 66, 52, and 21). Both priors are improper and 
cannot be found from Stan. 
You can use instead
%
\begin{align}
p(\mu) &\sim N(0,(10^3)^2)\nonumber\\ 
p(\sigma^2) &\sim \text{Inv-}\chi^2(\nu=4,s^2=1)  \label{eq:Inv-chi_prior}
\end{align}

In this exercise your tasks are the following:

 1. Write a Stan model for the above model and sample from the posterior of the parameters. Report the posterior mean, variance and 95\% central credible interval for $\mu$ and $\sigma^2$.
 2. Additionally draw samples from the posterior predictive distribution of hypothetical new measurement $p(\tilde{y}|y)$. Calculate the mean, variance and 95\% quantile of the posterior predictive distribution. 
 3. How does the posterior predictive distribution differ from the posterior 
of $\mu$ and Why? 
 4. Which parts of the model could be interpreted to correspond to aleatory and epistemic uncertainty? Discuss whether this distinction is useful here. 
 5. Instead of Inverse-$\chi^2$ distribution the variance parameter prior has traditionally been defined using Gamma distribution for the precision parameter $\tau=1/\sigma^2$. By using the results in Appendix A of BDA3 derive the analytic form of a Gamma prior for the precision corresponding to the prior \eqref{eq:Inv-chi_prior}. This should be of the form $\text{Gamma}(\alpha,\beta)$, where $\alpha$ and $\beta$ are functions of $\nu$ and $s^2$.

**Note!** Many common distributions have multiple parameterizations, for which 
reason you need to be careful when interpreting others' works. The 
variance/precision parameter and their priors are notorious for this. The reason 
is mainly historical since different parameterizations correspond to different 
analytical solutions.

**Grading:** 2 points from correct answer for each of the above steps.

## Model answers


Load the needed libraries into R and set options for multicore computer.
```{r}
library(ggplot2)
library(StanHeaders)
library(rstan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

y_vals = read.csv('./ex_speedOfLight.dat')$y

hist(y_vals, 40)
```

**Part 1. **

write the model description, set up initial values for 4 chains and sample from the posterior
```{r}
model = "
data {
    int N;
    vector[N] y; 
  }
parameters {
  real mu;
  real sigma2;
}
model {
  mu ~ normal(0, 10^3);
  sigma2 ~ inv_chi_square(4);
  y ~ normal(mu, sqrt(sigma2));
  
}
"

init1 = list(mu = 0, sigma2=5)
init2 = list(mu = 20, sigma2=30)
init3 = list(mu = 40, sigma2=20)
init4 = list(mu = 60, sigma2=15)
inits <- list(init1, init2, init3, init4) 

dataset=list(N=66, y=y_vals)

post=stan(model_code=model,data=dataset,warmup=500,iter=2000,chains=4,thin=1,init=inits,control = list(adapt_delta = 0.8,max_treedepth = 15))

```

```{r}
plot(post, plotfun="trace",pars="mu", inc_warmup=TRUE)
plot(post, plotfun="trace",pars="sigma2", inc_warmup=TRUE)
print(post, pars=c('sigma2', 'mu'))
stan_ac(post,inc_warmup = FALSE, lags = 25)
```
```{r}

```


**Part 2.**
```{r}
post_vals = as.matrix(post, pars=c('mu', 'sigma2'))
mus = post_vals[,1]
sigma2s = post_vals[,2]
size = length(mus)


mean(mus)
var(mus)
quantile(mus, c(0.025, 0.975))

mean(sigma2s)
var(sigma2s)
quantile(sigma2s, c(0.025, 0.975))

y_bar = rnorm(size, mus, sqrt(sigma2s))
```

```{r}
mean(y_bar)
var(y_bar)
quantile(y_bar, c(0.025, 0.975))
```
```{r}
hist(y_bar, 40)
hist(y_vals,40)
hist(mus,40)
```

**Part 3**

The posterior has a much larger spread on the x-axis, this is likely due due to the large values in the varaince parameter that allows for the distribution to encompass all the values in the dataset.

**Part 4**

We have aleatory uncertainty in that we have very few measurements, and by doing more measurements we could get a much better undertanding of the likely parameters of the real parameters for the phenomenon. This way we could find where our mean value should be with relatively high confidence so I would say that the mean of the distribution is what defines the aleatory uncertainty here. 

Also epistemic uncertainty exists, since our measurements are not precise and thus there is some variance and in this case the unit of measurement is very small so it can be difficult to minimize this variance. This epistemic uncertainty is modeled as the variance parameter in the distribution in this case.

**Part 5**

The inverse chi-square is an inverse-gamma distribution with the parameters of $\alpha = \nu / 2$ and $\beta = 1/2$. Then to conver the inverse gamma distribution into a gamma distribution we let $\beta = 1/\beta$, giving us $\beta = 2$, and finally giving us our distribution $\tau = Gamma(2, 2)$



